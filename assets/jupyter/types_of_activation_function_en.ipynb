{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Version of Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import jupyterlab as jlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions of Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Package</th>\n",
       "      <th>Version</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Torch</td>\n",
       "      <td>2.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NumPy</td>\n",
       "      <td>1.26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pandas</td>\n",
       "      <td>2.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JupyterLab</td>\n",
       "      <td>4.2.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Package Version\n",
       "#                    \n",
       "1       Torch   2.2.2\n",
       "2       NumPy  1.26.4\n",
       "3      Pandas   2.2.2\n",
       "4  JupyterLab   4.2.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "packages = [\n",
    "    \"Torch\", \"NumPy\", \"Pandas\", \"JupyterLab\",\n",
    "]\n",
    "\n",
    "package_objects = [\n",
    "    torch, np, pd, jlab\n",
    "]\n",
    "\n",
    "versions = [obj.__version__ for obj in package_objects]\n",
    "\n",
    "pkgs = {\"Package\": packages, \"Version\": versions}\n",
    "df_pkgs = pd.DataFrame(data = pkgs)\n",
    "df_pkgs.index.name = \"#\"\n",
    "df_pkgs.index += 1\n",
    "\n",
    "display(df_pkgs)\n",
    "\n",
    "path_to_reqs = \".\"\n",
    "reqs_name = \"requirements.txt\"\n",
    "\n",
    "def get_packages_and_versions():\n",
    "    \"\"\"Generate strings with libraries and their versions in the format: package==version\"\"\"\n",
    "    \n",
    "    for package, version in zip(packages, versions):\n",
    "        yield f\"{package.lower()}=={version}\\n\"\n",
    "\n",
    "with open(os.path.join(path_to_reqs, reqs_name), \"w\", encoding = \"utf-8\") as f:\n",
    "    f.writelines(get_packages_and_versions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "> Replace all negative values with 0, leaving positive values unchanged\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) =\n",
    "\\begin{cases} \n",
    "0 & \\text{if } x \\leq 0 \\\\\n",
    "x & \\text{if } x > 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([0.0000, 1.1458])\n"
     ]
    }
   ],
   "source": [
    "# Create a ReLU activation object\n",
    "g = nn.ReLU()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the ReLU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU (Exponential Linear Unit)\n",
    "\n",
    "> Convert negative values to $\\alpha (\\exp(x) - 1)$, making them less aggressive than *ReLU*, while leaving positive values unchanged\n",
    "\n",
    "$$\n",
    "\\text{ELU}(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-0.9049,  1.1458])\n"
     ]
    }
   ],
   "source": [
    "# Create a ELU activation object\n",
    "g = nn.ELU(alpha = 1.0)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the ELU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PReLU (Parametric ReLU)\n",
    "\n",
    "> Value conversion, leaving positive values unchanged and multiplying negative values by the training parameter $\\alpha$\n",
    "> $\\text{where } \\alpha \\text{ - trained parameter}$\n",
    "\n",
    "$$\n",
    "\\text{PReLU}(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha x & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([[ 0.6465, -0.9450, -0.5559, -1.5250],\n",
      "        [-1.4968, -1.1030,  0.5872, -0.7036]])\n",
      "Output data: tensor([[ 0.6465, -0.2362, -0.1390, -0.3812],\n",
      "        [-0.3742, -0.2758,  0.5872, -0.1759]], grad_fn=<PreluKernelBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a PReLU activation object\n",
    "g = nn.PReLU(num_parameters = 4, init = 0.25)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([\n",
    "    [ 0.6465, -0.9450, -0.5559, -1.5250],\n",
    "    [-1.4968, -1.1030,  0.5872, -0.7036]\n",
    "]) # torch.randn(2, 4)\n",
    "\n",
    "# Apply the PReLU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyReLU\n",
    "\n",
    "> Convert values by leaving positive values unchanged and multiplying negative values by the factor $\\alpha$,\n",
    "> $\\text{where } \\alpha = \\text{negative_slope}$\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha x & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-0.0235,  1.1458])\n"
     ]
    }
   ],
   "source": [
    "# Create a LeakyReLU activation object\n",
    "g = nn.LeakyReLU(negative_slope = 0.01)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the LeakyReLU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU6\n",
    "\n",
    "> Convert values less than 0 to 0, greater than 6 to 6, and leave other values unchanged\n",
    "\n",
    "$$\n",
    "\\text{ReLU6}(x) = \\min(\\max(0, x), 6)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458,  6.2345])\n",
      "Output data: tensor([0.0000, 1.1458, 6.0000])\n"
     ]
    }
   ],
   "source": [
    "# Create a ReLU6 activation object\n",
    "g = nn.ReLU6()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458, 6.2345]) # torch.randn(3)\n",
    "\n",
    "# Apply the ReLU6 activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RReLU (Randomized Leaky ReLU)\n",
    "\n",
    "> Convert values by leaving positive values unchanged and multiplying negative values by a random factor $\\alpha$\n",
    "\n",
    "$$\n",
    "\\text{RReLU}(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha x & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458,  6.2345])\n",
      "Output data: tensor([-0.6054,  1.1458,  6.2345])\n"
     ]
    }
   ],
   "source": [
    "# Create a RReLU activation object\n",
    "g = nn.RReLU(lower = 0.125, upper = 0.333)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458, 6.2345]) # torch.randn(3)\n",
    "\n",
    "# Apply the RReLU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU (Scaled Exponential Linear Unit)\n",
    "\n",
    "> Convert values, leaving positive values unchanged and scaling them with the $scale$ parameter, and scaling negative values with the $\\alpha$\n",
    "> and $scale$ parameters $\\text{where } \\text{scale} \\approx 1.0507 \\text{ and } \\alpha \\approx 1.6733$\n",
    "\n",
    "$$\n",
    "\\text{SELU}(x) = \\text{scale} \\times (\\max(0, x) + \\min(0, \\alpha \\times (\\exp(x) - 1)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-1.5909,  1.2039])\n"
     ]
    }
   ],
   "source": [
    "# Create a SELU activation object\n",
    "g = nn.SELU()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the SELU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELU (Continuously Differentiable Exponential Linear Unit)\n",
    "\n",
    "> Convert negative values using the exponential function and the $\\alpha$ parameter, leaving positive values unchanged\n",
    "\n",
    "$$\n",
    "\\text{CELU}(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha \\times (\\exp(x / \\alpha) - 1) & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-0.9049,  1.1458])\n"
     ]
    }
   ],
   "source": [
    "# Create a CELU activation object\n",
    "g = nn.CELU(alpha = 1.0)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the CELU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "> Transform values by approximating them to a Gaussian distribution\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\times \\Phi(x) \\quad \\text{where } \\Phi(x) \\text{ - cumulative distribution function for normal distribution (}approximate=none\\text{)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = 0.5 \\times x \\times \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715 x^3\\right)\\right)\\right) \\quad \\text{where } approximate=tanh\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-0.0219,  1.0015])\n"
     ]
    }
   ],
   "source": [
    "# Create a GELU activation object\n",
    "g = nn.GELU(approximate = \"none\") # none | tanh\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the GELU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "> Convert values from 0 to 1\n",
    "\n",
    "$$\n",
    "\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([0.0869, 0.7587])\n"
     ]
    }
   ],
   "source": [
    "# Create a Sigmoid activation object\n",
    "g = nn.Sigmoid()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the Sigmoid activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiLU (Sigmoid Linear Unit)\n",
    "\n",
    "> Combining the values of $x$ with their sigmoid transform $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$$\n",
    "\\text{SiLU}(x) = x \\times \\sigma(x) \\quad \\text{where } \\sigma(x) = \\frac{1}{1 + e^{-x}} \\text{ - sigmoid function}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-0.2043,  0.8694])\n"
     ]
    }
   ],
   "source": [
    "# Create a SiLU activation object\n",
    "g = nn.SiLU()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the SiLU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogSigmoid\n",
    "\n",
    "> Convert values using the logarithm of a sigmoid function\n",
    "\n",
    "$$\n",
    "\\text{LogSigmoid}(x) = \\log \\left( \\frac{1}{1 + \\exp(-x)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-2.4435, -0.2761])\n"
     ]
    }
   ],
   "source": [
    "# Create a LogSigmoid activation object\n",
    "g = nn.LogSigmoid()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the LogSigmoid activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardsigmoid\n",
    "\n",
    "> Convert values less than or equal to $-3$ to 0, values greater than or equal to $3$ to 1, and values between $-3$ and $3$ to $\\frac{x}{6} + 0.5$\n",
    "\n",
    "$$\n",
    "\\text{Hardsigmoid}(x) = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } x \\leq -3 \\\\\n",
    "1 & \\text{if } x \\geq 3 \\\\\n",
    "\\frac{x}{6} + 0.5 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-3.3526,  3.1458, -2.0256,  1.7843])\n",
      "Output data: tensor([0.0000, 1.0000, 0.1624, 0.7974])\n"
     ]
    }
   ],
   "source": [
    "# Create a Hardsigmoid activation object\n",
    "g = nn.Hardsigmoid()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-3.3526, 3.1458, -2.0256, 1.7843]) # torch.randn(4)\n",
    "\n",
    "# Apply the Hardsigmoid activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "\n",
    "> Convert values from -1 to 1\n",
    "\n",
    "$$\n",
    "\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-0.9821,  0.8164])\n"
     ]
    }
   ],
   "source": [
    "# Create a Tanh activation object\n",
    "g = nn.Tanh()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the Tanh activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanhshrink\n",
    "\n",
    "> Value transformation where the hyperbolic $tanh$ is subtracted from the input values themselves\n",
    "\n",
    "$$\n",
    "\\text{Tanhshrink}(x) = x - \\tanh(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-1.3705,  0.3294])\n"
     ]
    }
   ],
   "source": [
    "# Create a Tanhshrink activation object\n",
    "g = nn.Tanhshrink()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the Tanhshrink activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardtanh\n",
    "\n",
    "> Convert values less than $min\\_val$ to $min\\_val$, values greater than $max\\_val$ to $max\\_val$, and leave values between $min\\_val$\n",
    "> and $max\\_val$ unchanged\n",
    "\n",
    "$$\n",
    "\\text{HardTanh}(x) = \n",
    "\\begin{cases} \n",
    "min\\_val & \\text{if } x < min\\_val \\\\\n",
    "max\\_val & \\text{if } x > max\\_val \\\\\n",
    "x & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-1.1383,  1.1630, -0.8715,  0.7228])\n",
      "Output data: tensor([-1.0000,  1.0000, -0.8715,  0.7228])\n"
     ]
    }
   ],
   "source": [
    "# Create a Hardtanh activation object\n",
    "g = nn.Hardtanh(min_val = -1.0, max_val = 1.0)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-1.1383, 1.1630, -0.8715, 0.7228]) # torch.randn(4)\n",
    "\n",
    "# Apply the Hardtanh activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardshrink\n",
    "\n",
    "> Convert values between $-\\lambda$ and $\\lambda$ inclusive to 0, and leave values less than $-\\lambda$ or greater than $\\lambda$ unchanged\n",
    "\n",
    "$$\n",
    "\\text{HardShrink}(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > \\lambda \\\\\n",
    "x & \\text{if } x < -\\lambda \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-2.3526,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Create a Hardshrink activation object\n",
    "g = nn.Hardshrink(lambd = 1.1458)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the Hardshrink activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardswish\n",
    "\n",
    "> Convert values less than or equal to $-3$ to 0, values greater than or equal to $3$ remain unchanged, and values between $-3$\n",
    "> and $3$ are converted using the function $x \\left(\\frac{x+3}{6}\\right)$\n",
    "\n",
    "$$\n",
    "\\text{Hardswish}(x) = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } x \\leq -3 \\\\\n",
    "x & \\text{if } x \\geq 3 \\\\\n",
    "x \\left(\\frac{x+3}{6}\\right) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-3.3526,  3.1458, -2.0256,  1.7843])\n",
      "Output data: tensor([-0.0000,  3.1458, -0.3290,  1.4228])\n"
     ]
    }
   ],
   "source": [
    "# Create a Hardswish activation object\n",
    "g = nn.Hardswish()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-3.3526, 3.1458, -2.0256, 1.7843]) # torch.randn(4)\n",
    "\n",
    "# Apply the Hardswish activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mish\n",
    "\n",
    "> Convert values by combining $x$ values with their $softplus$ transformations ($\\text{Softplus}(x) = \\frac{1}{\\beta} \\log(1 + \\exp(\\beta \\times x))$) and applying the hyperbolic tangent\n",
    "\n",
    "$$\n",
    "\\text{Mish}(x) = x \\times \\tanh(\\text{Softplus}(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([-0.2132,  1.0198])\n"
     ]
    }
   ],
   "source": [
    "# Create a Mish activation object\n",
    "g = nn.Mish()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the Mish activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softplus\n",
    "\n",
    "> Convert values by smoothing, providing a smooth transition between linear and non-linear regions, controlled by the $\\beta$ parameter\n",
    "\n",
    "$$\n",
    "\\text{Softplus}(x) = \\frac{1}{\\beta} \\log(1 + \\exp(\\beta \\times x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458])\n",
      "Output data: tensor([0.0909, 1.4219])\n"
     ]
    }
   ],
   "source": [
    "# Create a Softplus activation object\n",
    "g = nn.Softplus(beta = 1.0, threshold = 20.0)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458]) # torch.randn(2)\n",
    "\n",
    "# Apply the Softplus activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softshrink\n",
    "\n",
    "> Convert values by decreasing them by $\\lambda$ and setting them to zero if the absolute value is less than $\\lambda$\n",
    "\n",
    "$$\n",
    "\\text{Softshrink}(x) = \n",
    "\\begin{cases} \n",
    "x - \\lambda & \\text{if } x > \\lambda \\\\\n",
    "x + \\lambda & \\text{if } x < -\\lambda \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458,  0.4320, -0.3791])\n",
      "Output data: tensor([-1.8526,  0.6458,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Create a Softshrink activation object\n",
    "g = nn.Softshrink(lambd = 0.5)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458, 0.4320, -0.3791]) # torch.randn(4)\n",
    "\n",
    "# Apply the Softshrink activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softsign\n",
    "\n",
    "> Transform values by decreasing their magnitude, where the input values are divided by the sum of 1 and their absolute value\n",
    "\n",
    "$$\n",
    "\\text{Softsign}(x) = \\frac{x}{1 + |x|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458,  0.4320, -0.3791])\n",
      "Output data: tensor([-0.7017,  0.5340,  0.3017, -0.2749])\n"
     ]
    }
   ],
   "source": [
    "# Create a Softsign activation object\n",
    "g = nn.Softsign()\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458, 0.4320, -0.3791]) # torch.randn(4)\n",
    "\n",
    "# Apply the Softsign activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold\n",
    "\n",
    "> Value conversion, where values greater than the specified $threshold$ remain unchanged and values less than the threshold are replaced with the specified $value$\n",
    "\n",
    "$$\n",
    "\\text{Threshold}(x) = \n",
    "\\begin{cases}\n",
    "x & \\text{if } x > threshold \\\\\n",
    "value & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([-2.3526,  1.1458,  0.4320, -0.3791])\n",
      "Output data: tensor([20.0000,  1.1458,  0.4320, 20.0000])\n"
     ]
    }
   ],
   "source": [
    "# Create a Threshold activation object\n",
    "g = nn.Threshold(threshold = 0.1, value = 20)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([-2.3526, 1.1458, 0.4320, -0.3791]) # torch.randn(4)\n",
    "\n",
    "# Apply the Threshold activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLU (Gated Linear Unit)\n",
    "\n",
    "> A value transformation in which the data is divided into two halves by the last dimension, the first half is left unchanged, a sigmoid function is applied to the second half, and multiplication of these parts gives the result\n",
    "\n",
    "$$\n",
    "\\text{GLU}(x) = x_1 \\times \\sigma(x_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([[-0.0915,  0.2352],\n",
      "        [ 2.2440,  0.5817],\n",
      "        [ 0.4528,  0.6410],\n",
      "        [ 0.5200,  0.5567]])\n",
      "Output data: tensor([[-0.0511],\n",
      "        [ 1.4394],\n",
      "        [ 0.2966],\n",
      "        [ 0.3306]])\n"
     ]
    }
   ],
   "source": [
    "# Create a GLU activation object\n",
    "g = nn.GLU(dim = -1)\n",
    "\n",
    "# Create a random tensor\n",
    "input = torch.tensor([\n",
    "    [-0.0915,  0.2352],\n",
    "    [ 2.2440,  0.5817],\n",
    "    [ 0.4528,  0.6410],\n",
    "    [ 0.5200,  0.5567]\n",
    "]) # torch.randn(4, 2)\n",
    "\n",
    "# Apply the GLU activation function to the input data\n",
    "output = g(input)\n",
    "\n",
    "# Print the input and output data\n",
    "print(\"Input data:\", input)\n",
    "print(\"Output data:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiheadAttention\n",
    "\n",
    "> Value conversion using a multi-headed attention mechanism, where the $query$, $key$, and $value$ inputs are passed through multiple heads of attention, combined, and linearly transformed into a final result\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O \\quad \\text{where } \\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result: tensor([[[-0.1921, -0.1406,  0.0313],\n",
      "         [-0.3418, -0.2391,  0.0791]],\n",
      "\n",
      "        [[-0.5842, -0.3964,  0.1620],\n",
      "         [-0.5963, -0.4015,  0.1717]],\n",
      "\n",
      "        [[-0.0468, -0.1383, -0.2063],\n",
      "         [-0.0530, -0.1416, -0.2026]]], grad_fn=<TransposeBackward0>)\n",
      "Attention Weights: tensor([[[0.5771, 0.4229],\n",
      "         [0.4379, 0.5621]],\n",
      "\n",
      "        [[0.4926, 0.5074],\n",
      "         [0.5116, 0.4884]],\n",
      "\n",
      "        [[0.4748, 0.5252],\n",
      "         [0.4837, 0.5163]]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "embed_dim = 3 # Embed dimension\n",
    "num_heads = 1 # Number of heads\n",
    "\n",
    "# Create a MultiheadAttention object\n",
    "multihead_attn = nn.MultiheadAttention(\n",
    "    embed_dim = embed_dim,\n",
    "    num_heads = num_heads,\n",
    "    dropout = 0.0,\n",
    "    bias = True,\n",
    "    add_bias_kv = False,\n",
    "    batch_first = True\n",
    ")\n",
    "\n",
    "# Create a random tensor for query, key Ð¸ value\n",
    "query = torch.tensor([\n",
    "    [\n",
    "        [-1.4025,  0.4318,  0.3431],\n",
    "        [ 1.0711,  1.3455,  0.3277]],\n",
    "    [\n",
    "        [ 1.3409,  1.2159,  0.9589],\n",
    "        [ 0.5137,  0.4977, -0.6646]],\n",
    "    [\n",
    "        [-1.0612,  2.0423,  0.6509],\n",
    "        [-1.0072,  0.3578, -1.0799]\n",
    "    ]\n",
    "]) # (batch size, target sequence length, embed_dim)\n",
    "key = torch.tensor([\n",
    "    [\n",
    "        [-1.4025,  0.4318,  0.3431],\n",
    "        [ 1.0711,  1.3455,  0.3277]],\n",
    "    [\n",
    "        [ 1.3409,  1.2159,  0.9589],\n",
    "        [ 0.5137,  0.4977, -0.6646]],\n",
    "    [\n",
    "        [-1.0612,  2.0423,  0.6509],\n",
    "        [-1.0072,  0.3578, -1.0799]\n",
    "    ]\n",
    "]) # (batch size, source sequence length, embed_dim)\n",
    "value = torch.tensor([\n",
    "    [\n",
    "        [-1.4025,  0.4318,  0.3431],\n",
    "        [ 1.0711,  1.3455,  0.3277]],\n",
    "    [\n",
    "        [ 1.3409,  1.2159,  0.9589],\n",
    "        [ 0.5137,  0.4977, -0.6646]],\n",
    "    [\n",
    "        [-1.0612,  2.0423,  0.6509],\n",
    "        [-1.0072,  0.3578, -1.0799]\n",
    "    ]\n",
    "]) # (batch size, source sequence length, embed_dim)\n",
    "\n",
    "# Apply the MultiheadAttention activation function to the input data\n",
    "# attn_output = (batch size, target sequence length, embed_dim)\n",
    "# attn_output_weights = (batch size, target sequence length, source sequence length)\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "# Print the output data\n",
    "print(\"Attention result:\", attn_output)\n",
    "print(\"Attention Weights:\", attn_output_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
