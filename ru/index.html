<!DOCTYPE html> <html lang=""> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Dr. Dmitry Ryumin </title> <meta name="author" content="Dr. Dmitry Ryumin"> <meta name="description" content="к.т.н. Рюмин Дмитрий Александрович - профессиональный профиль. Узнайте больше о его опыте, достижениях и вкладе в научное сообщество."> <meta name="keywords" content="dmitry-ryumin, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?f1b415aa808ffeb12b96aae400870343"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dmitryryumin.github.io/ru/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6E%65%77%65%72%61%61%69%72%65%73%65%61%72%63%68@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://telegram.me/dmitry_ryumin" title="telegram" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-telegram"></i></a> <a href="https://orcid.org/0000-0002-7935-0569" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=LrTIp5IAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/19199811" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://publons.com/a/K-7989-2018/" title="Publons" rel="external nofollow noopener" target="_blank"><i class="ai ai-publons"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=57191960214" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://github.com/DmitryRyumin" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="/ru/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/ru/">обо мне <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/ru/blog/">блог </a> </li> <li class="nav-item "> <a class="nav-link" href="/ru/publications/">публикации </a> </li> <li class="nav-item "> <a class="nav-link" href="/ru/repositories/">репозитории </a> </li> <li class="nav-item "> <a class="nav-link" href="/ru/cv/">резюме </a> </li> <li class="nav-item"> <button id="search-toggle" title="Поиск" onclick="openSearchModal()"> <span class="nav-link"><span id="search-hotkey"></span> <i class="ti ti-search"></i></span> </button> </li> <li class="nav-item active"> <a class="nav-link" href="/"> EN</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Изменить тему"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">к.т.н. Рюмин Дмитрий Александрович</span> </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/DmitryRyumin.jpg?94635fd45b4cfecf98238b19cf0185b6" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="DmitryRyumin.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p><img class="readme-typing-svg" src="https://readme-typing-svg.demolab.com?font=Roboto&amp;duration=1500&amp;pause=100&amp;color=2698ba&amp;vCenter=true&amp;multiline=true&amp;width=650&amp;height=95&amp;lines=%D0%98%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C+%D0%B2+%D0%BE%D0%B1%D0%BB%D0%B0%D1%81%D1%82%D0%B8+%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE+%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D0%B0;%D0%9A%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D0%BD%D0%BE%D0%B5+%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D0%B5+|+%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5+%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5;%D0%98%D0%BD%D1%82%D0%B5%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%BD%D1%8B%D0%B5+%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B+|+%D0%9C%D1%83%D0%BB%D1%8C%D1%82%D0%B8%D0%BC%D0%BE%D0%B4%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5+%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D1%84%D0%B5%D0%B9%D1%81%D1%8B" alt="РюминДмитрий"></p> <h2 class="d-flex align-items-center align-content-baseline"> <img class="width-32 mr-2" src="/assets/img/community_services.svg" alt="Научная коммуникация"> Научная коммуникация </h2> <h4 class="d-flex align-items-center align-content-baseline">Рецензент международных журналов</h4> <p> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/ESWA.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Expert%20Systems%20with%20Applications-28-045877?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/KNOSYS.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Knowledge--Based%20Systems-13-EFE30E?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/HLY.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Heliyon-13-027DBC?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/IMAVIS.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Image%20and%20Vision%20Computing-11-505050?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/NEUCOM.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Neurocomputing-11-EFE30E?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/EAAI.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Engineering%20Applications%20of%20Artificial%20Intelligence-11-583676?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/CAEE.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Computers%20and%20Electrical%20Engineering-10-EFE30E?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/PR.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Pattern%20Recognition-9-E87324?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/DIB.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Data%20in%20Brief-9-E11E27?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/IJCCE.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/International%20Journal%20of%20Cognitive%20Computing%20in%20Engineering-5-6BAFCC?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/NN.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Neural%20Networks-5-1E3887?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/ISWA.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Intelligent%20Systems%20with%20Applications-5-1C346D?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/PATREC.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Pattern%20Recognition%20Letters-4-000000?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/INFFUS.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Information%20Fusion-4-505050?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/YCVIU.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Computer%20Vision%20and%20Image%20Understanding-3-8A6734?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://www.webofscience.com/wos/author/record/K-7989-2018" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/IEEE%20Transactions%20on%20Circuits%20and%20Systems%20for%20Video%20Technology-3-14303E?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/SOFTX.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/SoftwareX-2-53C0AF?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/VISINF.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Visual%20Informatics-2-5E1918?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/SPECOM.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Speech%20Communication-2-DF1E25?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/NLP.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Natural%20Language%20Processing%20Journal-2-0F7C80?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/YCSLA.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Computer%20Speech%20and%20Language-2-227CC0?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/MEASUR.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Measurement-2-545CA8?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/ASOC.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Applied%20Soft%20Computing-2-1D3687?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/AQUE.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Aquacultural%20Engineering-1-53C0AF?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://github.com/DmitryRyumin/DmitryRyumin/blob/master/certificates/IPM.pdf" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Information%20Processing%20and%20Management-1-A6E1D7?&amp;style=flat-square" alt="Сертификат рецензента"> </a> <br> <a href="https://www.webofscience.com/wos/author/record/K-7989-2018" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/IEEE%20Transactions%20on%20Human--Machine%20Systems-1-0073AE?&amp;style=flat-square" alt="Сертификат рецензента"> </a> </p> <h4 class="d-flex align-items-center align-content-baseline">Рецензент международных конференций</h4> <p> <a href="https://interspeech2025.org/" rel="external nofollow noopener" target="_blank"> <img src="http://img.shields.io/badge/INTERSPEECH-2024%E2%80%9325-0C1C43.svg?&amp;style=flat-square" alt="INTERSPEECH"> </a> <a href="https://specom2024.ftn.uns.ac.rs/" rel="external nofollow noopener" target="_blank"> <img src="http://img.shields.io/badge/SPECOM-2023%E2%80%9324-FDD944.svg?&amp;style=flat-square" alt="SPECOM"> </a> </p> </div> <h2 class="d-flex align-items-center align-content-baseline"> <img class="width-32 mr-2" src="/assets/img/selected_posts.svg"> <a class="height-32" href="/ru/blog/" style="color: inherit">Последние заметки</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%"> 16 Июл., 2024</th> <td> <a class="news-title" href="/ru/blog/2024/intro-to-datasets/">Введение в библиотеку Datasets</a> </td> </tr> <tr> <th scope="row" style="width: 20%"> 05 Июл., 2024</th> <td> <a class="news-title" href="/ru/blog/2024/intro-to-deep-learning/">Введение в глубокое машинное обучение</a> </td> </tr> </table> </div> </div> <h2 class="d-flex align-items-center align-content-baseline"> <img class="width-32 mr-2" src="/assets/img/selected_papers.svg"> <a class="height-32" href="/ru/publications/" style="color: inherit">Избранные публикации</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div id="ryumina25_prl" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#000000"> <a href="https://www.sciencedirect.com/journal/pattern-recognition-letters" rel="external nofollow noopener" target="_blank">PRL</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/ryumina25_prl.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="ryumina25_prl.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ryumina25_prl" class="col-sm-9"> <div class="title">Multi-Corpus Emotion Recognition Method based on Cross-Modal Gated Attention Fusion</div> <div class="author"> <a href="https://scholar.google.com/citations?user=DOBkQssAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Elena Ryumina</a>, <em>Dmitry Ryumin</em>, <a href="https://hci.nw.ru/en/employees/9" rel="external nofollow noopener" target="_blank">Alexandr Axyonov</a>, <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>Pattern Recognition Letters</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.patrec.2025.02.024" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0167865525000662" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/SMIL-SPCRAS/MER" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:4fKUyHm3Qg0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Automatic emotion recognition techniques are critical to natural human–computer interaction. However, current methods suffer from limited applicability due to their tendency to overfit on single-corpus datasets. It reduces real-world effectiveness of the methods when faced with new unseen corpora. We propose the first multi-corpus multimodal emotion recognition method with high generalizability evaluated through a leave-one-corpus-out protocol. The method uses three fine-tuned encoders per modality (audio, video, and text) and a decoder employing a context-independent gated attention to combine features from all three modalities. The research is conducted on four benchmark corpora: MOSEI, MELD, IEMOCAP, and AFEW. The proposed method achieves the state-of-the-art results on these corpora and establishes the first baseline for multi-corpus studies. We demonstrate that due to the MELD rich emotional expressiveness across three modalities, the models trained on it exhibit the best generalization ability when applied to other corpora used. We also reveal that the AFEW annotation better correlates with the annotations of MOSEI, MELD, and IEMOCAP, as well as shows the best cross-corpus performance as it is consistent with the widely-accepted theories of basic emotions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ryumina25_prl</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ryumina, Elena and Ryumin, Dmitry and Axyonov, Alexandr and Ivanko, Denis and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Multi-Corpus Emotion Recognition Method based on Cross-Modal Gated Attention Fusion}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{190}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{192--200}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Pattern Recognition Letters}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0167-8655}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.patrec.2025.02.024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0167865525000662}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Multimodal Emotion Recognition, Encoders-Decoder, Context-Independent Features, Gated Feature Fusion, Multi-Corpus Study, Affective Computing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="ryumina24_interspeech" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#0C1C43"> <a href="https://www.isca-speech.org/Interspeech" rel="external nofollow noopener" target="_blank">INTERSPEECH</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/ryumina24_interspeech.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="ryumina24_interspeech.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ryumina24_interspeech" class="col-sm-9"> <div class="title">OCEAN-AI: Open Multimodal Framework for Personality Traits Assessment and HR-Processes Automatization</div> <div class="author"> <a href="https://scholar.google.com/citations?user=DOBkQssAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Elena Ryumina</a>, <em>Dmitry Ryumin</em>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>In Proceedings of the ISCA International Conference INTERSPEECH</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-archive.org/interspeech_2024/ryumina24_interspeech.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.isca-archive.org/interspeech_2024/ryumina24_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:l7t_Zn2s7bgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-pmid="" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Human personality traits (PT) reflect individual differences in patterns of thinking, feeling, and behaving. Knowledge on PT may be useful in many applied tasks in our everyday live. In this paper, we present a first open-source multimodal framework called OCEAN-AI for PT assessment (PTA) and HR-processes automatization. Our framework performs PTA analyzing three modalities, including audio, video, and text, and includes three processing modules. All the modules extract heterogeneous (deep neural and hand-crafted) features and use them for a com- plex analysis of human’s behavior. The final fourth module aggregates these six feature sets by a Siamese neural network with a gated attention mechanism. Our framework was tested on two free-available corpora, including First Impressions v2 and our MuPTA, and achieved the best results. Applying our framework, a user can automate solutions of some practical applied tasks, such as ranking potential candidates by professional responsibilities, forming efficient work teams and so on.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ryumina24_interspeech</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ryumina, Elena and Ryumin, Dmitry and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{OCEAN-AI: Open Multimodal Framework for Personality Traits Assessment and HR-Processes Automatization}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ISCA International Conference INTERSPEECH}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3630--3631}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.isca-archive.org/interspeech_2024/ryumina24_interspeech.html}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Multimodal Paralinguistics, Analysis of Speaker Traits, Personality Traits Assessment, Multimodal System}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="ryumina24_prl" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#000000"> <a href="https://www.sciencedirect.com/journal/pattern-recognition-letters" rel="external nofollow noopener" target="_blank">PRL</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/ryumina24_prl.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="ryumina24_prl.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ryumina24_prl" class="col-sm-9"> <div class="title">Gated Siamese Fusion Network based on Multimodal Deep and Hand-Crafted Features for Personality Traits Assessment</div> <div class="author"> <a href="https://scholar.google.com/citations?user=DOBkQssAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Elena Ryumina</a>, <a href="https://scholar.google.com/citations?user=JxZzcJoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Maxim Markitantov</a>, <em>Dmitry Ryumin</em>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>Pattern Recognition Letters</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.patrec.2024.07.004" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0167865524002071" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/aimclub/OCEANAI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:738O_yMBCRsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>People tend to judge others assessing their personality traits relying on life experience. This fact is especially evident when making an informed hiring decision, which should consider not only skills, but also match a company’s values and culture. Based on this assumption, we use the Siamese Network (SN) for assessing five personality traits by pairwise analyzing and comparing people simultaneously. For this, we propose the OCEAN-AI framework based on Gated Siamese Fusion Network (GSFN), which comprises six modules and enables the fusion of hand-crafted and deep features across three modalities (video, audio, and text). We use the ChaLearn First Impressions v2 (FIv2) and Multimodal Personality Traits Assessment (MuPTA) corpora and identify that all six feature sets and their combinations due to different information content allow the framework to adjust to heterogeneous input data flexibly. The experimental results show that the pairwise comparison of people with the same or different Personality Traits (PT) during the training enhances the proposed framework performance. The framework outperforms the State-of-the-Art (SOTA) systems based on three modalities (video-face, audio and text) by the relative value of 1.3% (0.928 vs. 0.916) in terms of the mean accuracy (mACC) on the FIv2 corpus. We also outperform the SOTA system in terms of the Concordance Correlation Coefficient (CCC) by the relative value of 8.6% (0.667 vs. 0.614) using two modalities (video and audio) on the MuPTA corpus. We make our framework publicly available to integrate it into various applications such as recruitment, education, and healthcare.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ryumina24_prl</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ryumina, Elena and Markitantov, Maxim and Ryumin, Dmitry and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Gated Siamese Fusion Network based on Multimodal Deep and Hand-Crafted Features for Personality Traits Assessment}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{185}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{45--51}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Pattern Recognition Letters}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0167-8655}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.patrec.2024.07.004}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0167865524002071}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Deep Learning, Multimodal Paralinguistics, Multimodal Gated Fusion, Hand-Crafted and Deep Features, Personality Computing, Affective Computing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="ryumina24_cvprw" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#1C427D"> <a href="https://cvpr.thecvf.com/" rel="external nofollow noopener" target="_blank">CVPRW</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/ryumina24_cvprw.jpg" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="ryumina24_cvprw.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ryumina24_cvprw" class="col-sm-9"> <div class="title">Zero-Shot Audio-Visual Compound Expression Recognition Method based on Emotion Probability Fusion</div> <div class="author"> <a href="https://scholar.google.com/citations?user=DOBkQssAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Elena Ryumina</a>, <a href="https://scholar.google.com/citations?user=JxZzcJoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Maxim Markitantov</a>, <em>Dmitry Ryumin</em>, <a href="https://scholar.google.com/citations?user=mPI3SpkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Heysem Kaya</a>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://elenaryumina.github.io/AVCER/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openaccess.thecvf.com/content/CVPR2024W/ABAW/papers/Ryumina_Zero-Shot_Audio-Visual_Compound_Expression_Recognition_Method_based_on_Emotion_Probability_CVPRW_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:Tiz5es2fbqcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-10-4285F4?logo=googlescholar&amp;labelColor=beige" alt="10 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-pmid="" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>A Compound Expression Recognition (CER) as a subfield of affective computing is a novel task in intelligent human-computer interaction and multimodal user interfaces. We propose a novel audio-visual method for CER. Our method relies on emotion recognition models that fuse modalities at the emotion probability level while decisions regarding the prediction of compound expressions are based on the pair-wise sum of weighted emotion probability distributions. Notably our method does not use any training data specific to the target task. Thus the problem is a zero-shot classification task. The method is evaluated in multi-corpus training and cross-corpus validation setups. We achieved F1 scores of 32.15% and 25.56% for the AffWild2 and C-EXPR-DB test subsets without training on target corpus and target task respectively. Therefore our method is on par with methods developed training target corpus or target task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ryumina24_cvprw</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ryumina, Elena and Markitantov, Maxim and Ryumin, Dmitry and Kaya, Heysem and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Zero-Shot Audio-Visual Compound Expression Recognition Method based on Emotion Probability Fusion}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4752--4760}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openaccess.thecvf.com/content/CVPR2024W/ABAW/html/Ryumina_Zero-Shot_Audio-Visual_Compound_Expression_Recognition_Method_based_on_Emotion_Probability_CVPRW_2024_paper.html}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="Ruimin2024AVCRFormer" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#045877"> <a href="https://www.sciencedirect.com/journal/expert-systems-with-applications" rel="external nofollow noopener" target="_blank">ESWA</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/Ruimin2024AVCRFormer.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="Ruimin2024AVCRFormer.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ruimin2024AVCRFormer" class="col-sm-9"> <div class="title">Audio-Visual Speech Recognition based on Regulated Transformer and Spatio-Temporal Fusion Strategy for Driver Assistive Systems</div> <div class="author"> <em>Dmitry Ryumin</em>, <a href="https://hci.nw.ru/en/employees/9" rel="external nofollow noopener" target="_blank">Alexandr Axyonov</a>, <a href="https://scholar.google.com/citations?user=DOBkQssAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Elena Ryumina</a>, <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, <a href="https://scholar.google.com/citations?user=XAwrzbYAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Kashevnik</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alexey Karpov' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Expert Systems with Applications</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.eswa.2024.124159" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S095741742401025X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:XiSMed-E-HIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-14-4285F4?logo=googlescholar&amp;labelColor=beige" alt="14 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2024.124159" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This article presents a research methodology for audio-visual speech recognition (AVSR) in driver assistive systems. These systems necessitate ongoing interaction with drivers while driving through voice control for safety reasons. The article introduces a novel audio-visual speech command recognition transformer (AVCRFormer) specifically designed for robust AVSR. We propose (i) a multimodal fusion strategy based on spatio-temporal fusion of audio and video feature matrices, (ii) a regulated transformer based on iterative model refinement module with multiple encoders, (iii) a classifier ensemble strategy based on multiple decoders. The spatio-temporal fusion strategy preserves contextual information of both modalities and achieves their synchronization. An iterative model refinement module can bridge the gap between acoustic and visual data by leveraging their impact on speech recognition accuracy. The proposed multi-prediction strategy demonstrates superior performance compared to traditional single-prediction strategy, showcasing the model’s adaptability across diverse audio-visual contexts. The transformer proposed has achieved the highest values of speech command recognition accuracy, reaching 98.87% and 98.81% on the RUSAVIC and LRW corpora, respectively. This research has significant implications for advancing human–computer interaction. The capabilities of AVCRFormer extend beyond AVSR, making it a valuable contribution to the intersection of audio-visual processing and artificial intelligence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ruimin2024AVCRFormer</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ryumin, Dmitry and Axyonov, Alexandr and Ryumina, Elena and Ivanko, Denis and Kashevnik, Alexey and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Audio-Visual Speech Recognition based on Regulated Transformer and Spatio-Temporal Fusion Strategy for Driver Assistive Systems}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Expert Systems with Applications}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{124159}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.eswa.2024.124159}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S095741742401025X}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Audio-Visual Speech Recognition, Spatio-Temporal Fusion Strategy, Classifier Ensemble, Transformer, Computer Vision, Driver Assistive Systems}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="Ruimina2024OCEAN-AI" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#045877"> <a href="https://www.sciencedirect.com/journal/expert-systems-with-applications" rel="external nofollow noopener" target="_blank">ESWA</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/Ruimina2024OCEAN-AI.jpg" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="Ruimina2024OCEAN-AI.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ruimina2024OCEAN-AI" class="col-sm-9"> <div class="title">OCEAN-AI Framework with EmoFormer Cross-Hemiface Attention Approach for Personality Traits Assessment</div> <div class="author"> <a href="https://scholar.google.com/citations?user=DOBkQssAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Elena Ryumina</a>, <a href="https://scholar.google.com/citations?user=JxZzcJoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Maxim Markitantov</a>, <em>Dmitry Ryumin</em>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>Expert Systems with Applications</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.eswa.2023.122441" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417423029433" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:UxriW0iASnsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-10-4285F4?logo=googlescholar&amp;labelColor=beige" alt="10 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2023.122441" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Psychological and neurological studies earlier suggested that a personality type can be determined by the whole face as well as by its sides. This article discusses novel research using deep neural networks that address the features of both sides of the face (hemifaces) to assess the human’s Big Five personality traits (PT). For this, we have developed a real-time approach called EmoFormer with cross-hemiface attention. The novelty of the presented approach lies in the confirmation that each hemiface exhibits high predictive capabilities in terms of human’s PT distinction. Our approach is based on a novel mid-level emotional feature extractor for each hemiface and a cross-hemiface attention fusion strategy for hemiface feature aggregation. The consequent fusion of both hemifaces has outperformed the use of the whole face by the relative value of 3.6% in terms of Concordance Correlation Coefficient (0.634 vs. 0.612) on the ChaLearn First Impressions V2 corpus. The proposed approach has also outperformed all the existing state-of-the-art approaches for PT assessment based on the face modality. We have also analyzed the “best hemiface”, the one that predicts PT more accurately in terms of demographic characteristics (gender, ethnicity, and age). We have found that the best hemiface for two of the five PT (Openness to experience and Non-Neuroticism) is different depending on demographic characteristics. For the other three traits, the right hemiface is dominant for Extraversion, while the left one is more indicative of Conscientiousness and Agreeableness. These findings support previous psychological and neurological research. Besides, we provide an open-source framework referred to as OCEAN-AI that can be seamlessly integrated into expert systems with practical applications in various domains including healthcare, education, and human resources.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ruimina2024OCEAN-AI</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ryumina, Elena and Markitantov, Maxim and Ryumin, Dmitry and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{OCEAN-AI Framework with EmoFormer Cross-Hemiface Attention Approach for Personality Traits Assessment}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Expert Systems with Applications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{239}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{122441}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0957-4174}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.eswa.2023.122441}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0957417423029433}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Personality Computing, Big Five, Emotional Features, Hemifaces, Feature-Level Fusion, Deep Learning, Transformer}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="axyonov24_icassp" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#14303E"> <a href="https://ieeexplore.ieee.org/xpl/conhome/1000002/all-proceedings" rel="external nofollow noopener" target="_blank">ICASSP</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/axyonov24_icassp.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="axyonov24_icassp.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="axyonov24_icassp" class="col-sm-9"> <div class="title">Audio-Visual Speech Recognition In-the-Wild: Multi-Angle Vehicle Cabin Corpus and Attention-based Method</div> <div class="author"> <a href="https://hci.nw.ru/en/employees/9" rel="external nofollow noopener" target="_blank">Alexandr Axyonov</a>, <em>Dmitry Ryumin</em>, <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, <a href="https://scholar.google.com/citations?user=XAwrzbYAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Kashevnik</a>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICASSP48485.2024.10448048" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10448048" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:u9iWguZQMMsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICASSP48485.2024.10448048" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Audio-visual speech recognition (AVSR) gains increasing attention as an important part of human-machine interaction. However, the publicly available corpora are limited, particularly in driving conditions with prevalent background noise. Research so far has been collected in constrained environments, and thus cannot reflect the true performance of AVSR systems in real-world scenarios. Moreover, data for languages other than English is often unavailable. To meet the request for research on AVSR in unconstrained driving conditions, this paper presents a corpus collected ‘in-the-wild’. We propose a cross-modal attention method enhancing multi-angle AVSR for vehicles, leveraging visual context to improve accuracy and noise robustness. Our proposed model achieves state-of-the-art (SOTA) results with 98.65% accuracy in recognizing driver voice commands.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">axyonov24_icassp</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Axyonov, Alexandr and Ryumin, Dmitry and Ivanko, Denis and Kashevnik, Alexey and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Audio-Visual Speech Recognition In-the-Wild: Multi-Angle Vehicle Cabin Corpus and Attention-based Method}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2379-190X}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8195--8199}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP48485.2024.10448048}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/10448048}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Human Computer Interaction, Visualization, Speech Recognition, Signal Processing,Benchmark Testing, Noise Robustness, Noise Measurement, Multi-Modal Signal Processing,Audio-Visual Speech Recognition, Attention Mechanism, Feature-Level Fusion, Spatio-Temporal Features}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="Ryumin2024s23042284" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#017C7F"> <a href="https://www.mdpi.com/journal/sensors" rel="external nofollow noopener" target="_blank">Sensors</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/Ryumin2024s23042284.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="Ryumin2024s23042284.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryumin2024s23042284" class="col-sm-9"> <div class="title">Audio-Visual Speech and Gesture Recognition by Sensors of Mobile Devices</div> <div class="author"> Dmitry Ryumin <sup>*†</sup>, <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, and Elena Ryumina <sup>†</sup> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Author to whom correspondence should be addressed&lt;br&gt;† These authors contributed equally to this work"> </i> </div> <div class="periodical"> <em>Sensors</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3390/s23042284" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.mdpi.com/1424-8220/23/4/2284" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:NhqRSupF_l8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-80-4285F4?logo=googlescholar&amp;labelColor=beige" alt="80 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-doi="10.3390/s23042284" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Audio-visual speech recognition (AVSR) is one of the most promising solutions for reliable speech recognition, particularly when audio is corrupted by noise. Additional visual information can be used for both automatic lip-reading and gesture recognition. Hand gestures are a form of non-verbal communication and can be used as a very important part of modern human–computer interaction systems. Currently, audio and video modalities are easily accessible by sensors of mobile devices. However, there is no out-of-the-box solution for automatic audio-visual speech and gesture recognition. This study introduces two deep neural network-based model architectures: one for AVSR and one for gesture recognition. The main novelty regarding audio-visual speech recognition lies in fine-tuning strategies for both visual and acoustic features and in the proposed end-to-end model, which considers three modality fusion approaches: prediction-level, feature-level, and model-level. The main novelty in gesture recognition lies in a unique set of spatio-temporal features, including those that consider lip articulation information. As there are no available datasets for the combined task, we evaluated our methods on two different large-scale corpora—LRW and AUTSL—and outperformed existing methods on both audio-visual speech recognition and gesture recognition tasks. We achieved AVSR accuracy for the LRW dataset equal to 98.76% and gesture recognition rate for the AUTSL dataset equal to 98.56%. The results obtained demonstrate not only the high performance of the proposed methodology, but also the fundamental possibility of recognizing audio-visual speech and gestures by sensors of mobile devices.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ryumin2024s23042284</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{, Dmitry Ryumin and Ivanko, Denis and , Elena Ryumina}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Audio-Visual Speech and Gesture Recognition by Sensors of Mobile Devices}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Sensors}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">article-number</span> <span class="p">=</span> <span class="s">{2284}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1424-8220}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/s23042284}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.mdpi.com/1424-8220/23/4/2284}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Audio-Visual Speech Recognition, Model-Level Fusion, Lip-Reading, Gesture Recognition, Spatio-Temporal Features, Dimensionality Reduction Technique, Computer Vision}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="ryumina23_interspeech" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#0C1C43"> <a href="https://www.isca-speech.org/Interspeech" rel="external nofollow noopener" target="_blank">INTERSPEECH</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/ryumina23_interspeech.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="ryumina23_interspeech.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ryumina23_interspeech" class="col-sm-9"> <div class="title">Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech</div> <div class="author"> <a href="https://scholar.google.com/citations?user=DOBkQssAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Elena Ryumina</a>, <em>Dmitry Ryumin</em>, <a href="https://scholar.google.com/citations?user=JxZzcJoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Maxim Markitantov</a>, <a href="https://scholar.google.com/citations?user=mPI3SpkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Heysem Kaya</a>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>In Proceedings of the ISCA International Conference INTERSPEECH</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.21437/Interspeech.2023-1686" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:KxtntwgDAa4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-doi="10.21437/Interspeech.2023-1686" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Automatic personality traits assessment (PTA) provides high-level, intelligible predictive inputs for subsequent critical downstream tasks, such as job interview recommendations and mental healthcare monitoring. In this work, we introduce a novel Multimodal Personality Traits Assessment (MuPTA) corpus. Our MuPTA corpus is unique in that it contains both spontaneous and read speech collected in the midly-resourced Russian language. We present a novel audio-visual approach for PTA that is used in order to set up baseline results on this corpus. We further analyze the impact of both spontaneous and read speech types on the PTA predictive performance. We find that for the audio modality, the PTA predictive performances on short signals are almost equal regardless of the speech type, while PTA using video modality is more accurate with spontaneous speech compared to read one regardless of the signal length.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ryumina23_interspeech</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ryumina, Elena and Ryumin, Dmitry and Markitantov, Maxim and Kaya, Heysem and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ISCA International Conference INTERSPEECH}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4049--4053}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/Interspeech.2023-1686}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.html}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Audio-Visual Resources, Data Annotation, Multimodal Paralinguistics, Personality Computing, Big Five Traits}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="ivanko22_interspeech" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#0C1C43"> <a href="https://www.isca-speech.org/Interspeech" rel="external nofollow noopener" target="_blank">INTERSPEECH</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/ivanko22_interspeech.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="ivanko22_interspeech.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ivanko22_interspeech" class="col-sm-9"> <div class="title">DAVIS: Driver’s Audio-Visual Speech Recognition</div> <div class="author"> <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, <em>Dmitry Ryumin</em>, <a href="https://scholar.google.com/citations?user=XAwrzbYAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Kashevnik</a>, <a href="https://hci.nw.ru/en/employees/9" rel="external nofollow noopener" target="_blank">Alexandr Axyonov</a>, <a href="https://www.scopus.com/authid/detail.uri?authorId=57932540100" rel="external nofollow noopener" target="_blank">Andrey Kitenko</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Igor Lashkov, Alexey Karpov' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the ISCA International Conference INTERSPEECH</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-archive.org/interspeech_2022/ivanko22_interspeech.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.isca-archive.org/interspeech_2022/ivanko22_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:4OULZ7Gr8RgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-11-4285F4?logo=googlescholar&amp;labelColor=beige" alt="11 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-pmid="" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>DAVIS is a driver’s audio-visual assistive system intended to improve accuracy and robustness of speech recognition of the most frequent drivers’ requests in natural driving conditions. Since speech recognition in driving condition is highly challenging due to acoustic noises, active head turns, pose variation, distance to recording devices, lightning conditions, etc. We rely on multimodal information and use both automatic lip-reading system for visual stream and ASR for audio stream processing. We have trained audio and video models on own RUSAVIC dataset containing in-the-wild audio and video recordings of 20 drivers. The recognition application comprises a graphical user interface and modules for audio and video signal acquisition, analysis, and recognition. The obtained results demonstrate rather high performance of DAVIS and also the fundamental possibility of recognizing speech commands by using video modality, even in such difficult natural conditions as driving.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ivanko22_interspeech</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ivanko, Denis and Ryumin, Dmitry and Kashevnik, Alexey and Axyonov, Alexandr and Kitenko, Andrey and Lashkov, Igor and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{DAVIS: Driver’s Audio-Visual Speech Recognition}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ISCA International Conference INTERSPEECH}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1141--1142}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.isca-archive.org/interspeech_2022/ivanko22_interspeech.html}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Audio-Visual Speech Recognition, Driver Assistance System, Human-Computer Interaction}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="markitantov22_interspeech" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#0C1C43"> <a href="https://www.isca-speech.org/Interspeech" rel="external nofollow noopener" target="_blank">INTERSPEECH</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/markitantov22_interspeech.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="markitantov22_interspeech.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="markitantov22_interspeech" class="col-sm-9"> <div class="title">Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS) Corpus: Multimodal Mask Type Recognition Task</div> <div class="author"> <a href="https://scholar.google.com/citations?user=JxZzcJoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Maxim Markitantov</a>, <a href="https://scholar.google.com/citations?user=DOBkQssAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Elena Ryumina</a>, <em>Dmitry Ryumin</em>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>In Proceedings of the ISCA International Conference INTERSPEECH</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.21437/Interspeech.2022-10240" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.isca-archive.org/interspeech_2022/markitantov22_interspeech.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.isca-archive.org/interspeech_2022/markitantov22_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:fPk4N6BV_jEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-12-4285F4?logo=googlescholar&amp;labelColor=beige" alt="12 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-doi="10.21437/Interspeech.2022-10240" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In this paper, we present a new multimodal corpus called Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS), which is designed to analyze voice and facial characteristics of persons wearing various masks, as well as to develop automatic systems for bimodal verification and identification of speakers. In particular, we tackle the multimodal mask type recognition task (6 classes). As a result, audio, visual and multimodal systems were developed, which showed UAR of 54.83%, 72.02% and 82.01%, respectively, on the Test set. These performances are the baseline for the BRAVE-MASKS corpus to compare the follow-up approaches with the proposed systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">markitantov22_interspeech</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Markitantov, Maxim and Ryumina, Elena and Ryumin, Dmitry and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS) Corpus: Multimodal Mask Type Recognition Task}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ISCA International Conference INTERSPEECH}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1756--1760}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/Interspeech.2022-10240}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.isca-archive.org/interspeech_2022/markitantov22_interspeech.html}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Mask Type Recognition, Face Masks Detection, Computational Paralinguistics, Corpora Annotation, Data Augmentation, Machine Learning, COVID-19}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="ivanko22_icmi" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#672B83"> <a href="https://icmi.acm.org" rel="external nofollow noopener" target="_blank">ICMI</a> </abbr> </div> <div id="ivanko22_icmi" class="col-sm-9"> <div class="title">MIDriveSafely: Multimodal Interaction for Drive Safely</div> <div class="author"> <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, <a href="https://scholar.google.com/citations?user=XAwrzbYAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Kashevnik</a>, <em>Dmitry Ryumin</em>, <a href="https://www.scopus.com/authid/detail.uri?authorId=57932540100" rel="external nofollow noopener" target="_blank">Andrey Kitenko</a>, <a href="https://hci.nw.ru/en/employees/9" rel="external nofollow noopener" target="_blank">Alexandr Axyonov</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Igor Lashkov, Alexey Karpov' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the International Conference on Multimodal Interaction (ICMI)</em>, Bengaluru, India, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3536221.3557037" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/3536221.3557037" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:D03iK_w7-QYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-doi="10.1145/3536221.3557037" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In this paper, we present a novel multimodal interaction application to help car drivers and increase their road safety. MIDriveSafely is a mobile application that provides the following functions: (1) detect dangerous situations based on video information from a smartphone front-facing camera, such as drowsiness/sleepiness, phone usage while driving, eating, smoking, unfastened seat belt, etc.; gives a feedback to the driver (2) provide entertainment (e.g. rock-paper-scissors game, based on automatic speech recognition), (3) provide voice control capabilities to navigation/multimedia systems of a smartphone (potentially vehicle systems such as lighting conditions/climate control). Speech recognition in driving conditions is highly challenging due to acoustic noises, active head turns, pose variations, distance to recording devices, etc. MIDriveSafely incorporates driver’s audio-visual speech recognition (DAVIS) system and uses it for multimodal interaction. Along with this, the original DriveSafely system is used for dangerous state detection. MIDriveSafely improves upon existing driver monitoring applications using multimodal (mainly audio-visual) information. MIDriveSafely motivates people to drive in a safer manner by providing the feedback to the drivers and by creating a fun user experience.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ivanko22_icmi</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ivanko, Denis and Kashevnik, Alexey and Ryumin, Dmitry and Kitenko, Andrey and Axyonov, Alexandr and Lashkov, Igor and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{MIDriveSafely: Multimodal Interaction for Drive Safely}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Multimodal Interaction (ICMI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{733--735}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450393904}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Bengaluru, India}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3536221.3557037}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3536221.3557037}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Driver Monitoring, Mobile Multimodal Systems, Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="ivanko22_eusipco" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#37406B"> <a href="https://eurasip.org/eusipco-conferences/" rel="external nofollow noopener" target="_blank">EUSIPCO</a> </abbr> </div> <div id="ivanko22_eusipco" class="col-sm-9"> <div class="title">Visual Speech Recognition in a Driver Assistance System</div> <div class="author"> <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, <em>Dmitry Ryumin</em>, <a href="https://scholar.google.com/citations?user=XAwrzbYAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Kashevnik</a>, <a href="https://hci.nw.ru/en/employees/9" rel="external nofollow noopener" target="_blank">Alexandr Axyonov</a>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>In Proceedings of the European Signal Processing Conference (EUSIPCO)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.23919/EUSIPCO55093.2022.9909819" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9909819" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:rO6llkc54NcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-30-4285F4?logo=googlescholar&amp;labelColor=beige" alt="30 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-doi="10.23919/EUSIPCO55093.2022.9909819" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Visual speech recognition or automated lip-reading is a field of growing attention. Video data proved its usefulness in multimodal speech recognition, especially when acoustic data is heavily noised or even inaccessible. In this paper, we present a novel method for visual speech recognition. We benchmark it on the famous LRW lip-reading dataset by outperforming the existing approaches. After a comprehensive evaluation, we adapt the developed method and test it on the collected RUSAVIC corpus we recorded in-the-wild for vehicle driver. The results obtained demonstrate not only the high performance of the proposed method, but also the fundamental possibility of recognizing speech only by using video modality, even in such difficult natural conditions as driving.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ivanko22_eusipco</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ivanko, Denis and Ryumin, Dmitry and Kashevnik, Alexey and Axyonov, Alexandr and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Visual Speech Recognition in a Driver Assistance System}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the European Signal Processing Conference (EUSIPCO)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1131--1135}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9909819}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.23919/EUSIPCO55093.2022.9909819}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Visualization, Europe, Speech Recognition, Benchmark Testing, Signal Processing,Acoustics, Speech Processing, Visual Speech Recognition, Automated Lip-Reading, End-to-End, Speech Recognition, Computer Vision}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="ivanko2022lrec" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#81026E"> <a href="https://aclanthology.org/venues/lrec/" rel="external nofollow noopener" target="_blank">LREC</a> </abbr> </div> <div id="ivanko2022lrec" class="col-sm-9"> <div class="title">RUSAVIC Corpus: Russian Audio-Visual Speech in Cars</div> <div class="author"> <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, <a href="https://hci.nw.ru/en/employees/9" rel="external nofollow noopener" target="_blank">Alexandr Axyonov</a>, <em>Dmitry Ryumin</em>, <a href="https://scholar.google.com/citations?user=XAwrzbYAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Kashevnik</a>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>In Proceedings of the Language Resources and Evaluation Conference (LREC)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.lrec-1.166/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2022.lrec-1.166.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:ZHo1McVdvXMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-24-4285F4?logo=googlescholar&amp;labelColor=beige" alt="24 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-pmid="" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We present a new audio-visual speech corpus (RUSAVIC) recorded in a car environment and designed for noise-robust speech recognition. Our goal was to produce a speech corpus which is natural (recorded in real driving conditions), controlled (providing different SNR levels by windows open/closed, moving/parked vehicle, etc.), and adequate size (the amount of data is enough to train state-of-the-art NN approaches). We focus on the problem of audio-visual speech recognition: with the use of automated lip-reading to improve the performance of audio-based speech recognition in the presence of severe acoustic noise caused by road traffic. We also describe the equipment and procedures used to create RUSAVIC corpus. Data are collected in a synchronous way through several smartphones located at different angles and equipped with FullHD video camera and microphone. The corpus includes the recordings of 20 drivers with minimum of 10 recording sessions for each. Besides providing a detailed description of the dataset and its collection pipeline, we evaluate several popular audio and visual speech recognition methods and present a set of baseline recognition results. At the moment RUSAVIC is a unique audio-visual corpus for the Russian language that is recorded in-the-wild condition and we make it publicly available.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ivanko2022lrec</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ivanko, Denis and Axyonov, Alexandr and Ryumin, Dmitry and Kashevnik, Alexey and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{RUSAVIC Corpus: Russian Audio-Visual Speech in Cars}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Language Resources and Evaluation Conference (LREC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1555--1559}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Marseille, France}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{European Language Resources Association}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.lrec-1.166}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Audio-Visual Corpus, Automatic Speech Recognition, Data Collection, Automated Lip-Reading, Driver Monitoring}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="Kashevnik2021_9364986" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#14303E"> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639" rel="external nofollow noopener" target="_blank">IEEE Access</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/Kashevnik2021_9364986.png" class="preview z-depth-1 rounded-bottom" width="100%" height="auto" alt="Kashevnik2021_9364986.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kashevnik2021_9364986" class="col-sm-9"> <div class="title">Multimodal Corpus Design for Audio-Visual Speech Recognition in Vehicle Cabin</div> <div class="author"> <a href="https://scholar.google.com/citations?user=XAwrzbYAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Kashevnik</a>, <a href="https://scholar.google.com/citations?user=btDuT3gAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Igor Lashkov</a>, <a href="https://hci.nw.ru/en/employees/9" rel="external nofollow noopener" target="_blank">Alexandr Axyonov</a>, <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, <em>Dmitry Ryumin</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Artem Kolchin, Alexey Karpov' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>IEEE Access</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ACCESS.2021.3062752" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9364986" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9364986" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-33-4285F4?logo=googlescholar&amp;labelColor=beige" alt="33 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-doi="10.1109/ACCESS.2021.3062752" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>This paper introduces a new methodology aimed at comfort for the driver in-the-wild multimodal corpus creation for audio-visual speech recognition in driver monitoring systems. The presented methodology is universal and can be used for corpus recording for different languages. We present an analysis of speech recognition systems and voice interfaces for driver monitoring systems based on the analysis of both audio and video data. Multimodal speech recognition allows using audio data when video data are useless (e.g. at nighttime), as well as applying video data in acoustically noisy conditions (e.g., at highways). Our methodology identifies the main steps and requirements for multimodal corpus designing, including the development of a new framework for audio-visual corpus creation. We identify the main research questions related to the speech corpus creation task and discuss them in detail in this paper. We also consider some main cases of usage that require speech recognition in a vehicle cabin for interaction with a driver monitoring system. We also consider other important use cases when the system detects dangerous states of driver’s drowsiness and starts a question-answer game to prevent dangerous situations. At the end based on the proposed methodology, we developed a mobile application that allows us to record a corpus for the Russian language. We created RUSAVIC corpus using the developed mobile application that at the moment a unique audiovisual corpus for the Russian language that is recorded in-the-wild condition.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Kashevnik2021_9364986</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kashevnik, Alexey and Lashkov, Igor and Axyonov, Alexandr and Ivanko, Denis and Ryumin, Dmitry and Kolchin, Artem and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Multimodal Corpus Design for Audio-Visual Speech Recognition in Vehicle Cabin}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Access}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{34986--35003}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2169-3536}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ACCESS.2021.3062752}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9364986}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Vehicles, Speech Recognition, Smart Phones, Monitoring, Sensors, Vocabulary, Task Analysis, Driver Monitoring, Automatic Speech Recognition, Multimodal Corpus, Human–Computer Interaction}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div id="ivanko2022lred" class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-100 rounded-0" style="background-color:#81026E"> <a href="https://aclanthology.org/venues/lrec/" rel="external nofollow noopener" target="_blank">LREC</a> </abbr> </div> <div id="ivanko2022lred" class="col-sm-9"> <div class="title">TheRuSLan: Database of Russian Sign Language</div> <div class="author"> <a href="https://www.scopus.com/authid/detail.uri?authorId=25121369400" rel="external nofollow noopener" target="_blank">Ildar Kagirov</a>, <a href="https://scholar.google.com/citations?user=CyE3W0oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Denis Ivanko</a>, <em>Dmitry Ryumin</em>, <a href="https://hci.nw.ru/en/employees/9" rel="external nofollow noopener" target="_blank">Alexandr Axyonov</a>, and <a href="https://scholar.google.com/citations?user=Q0C3f1oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Alexey Karpov</a> </div> <div class="periodical"> <em>In Proceedings of the Language Resources and Evaluation Conference (LREC)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2020.lrec-1.746/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2020.lrec-1.746.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LrTIp5IAAAAJ&amp;citation_for_view=LrTIp5IAAAAJ:YsMSGLbcyi4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-30-4285F4?logo=googlescholar&amp;labelColor=beige" alt="30 Google Scholar citations"> </a> <span class="__dimensions_badge_embed__" data-pmid="" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In this paper, a new Russian sign language multimedia database TheRuSLan is presented. The database includes lexical units (single words and phrases) from Russian sign language within one subject area, namely, “food products at the supermarket”, and was collected using MS Kinect 2.0 device including both FullHD video and the depth map modes, which provides new opportunities for the lexicographical description of the Russian sign language vocabulary and enhances research in the field of automatic gesture recognition. Russian sign language has an official status in Russia, and over 120,000 deaf people in Russia and its neighboring countries use it as their first language. Russian sign language has no writing system, is poorly described and belongs to the low-resource languages. The authors formulate the basic principles of annotation of sign words, based on the collected data, and reveal the content of the collected database. In the future, the database will be expanded and comprise more lexical units. The database is explicitly made for the task of creating an automatic system for Russian sign language recognition.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ivanko2022lred</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kagirov, Ildar and Ivanko, Denis and Ryumin, Dmitry and Axyonov, Alexandr and Karpov, Alexey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{TheRuSLan: Database of Russian Sign Language}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Language Resources and Evaluation Conference (LREC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6079--6085}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Marseille, France}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{European Language Resources Association}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.lrec-1.746}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Russian Sign Language, Low Resourced Languages, Corpora Annotation, Image Recognition, Machine Learning}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6E%65%77%65%72%61%61%69%72%65%73%65%61%72%63%68@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://telegram.me/dmitry_ryumin" title="telegram" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-telegram"></i></a> <a href="https://orcid.org/0000-0002-7935-0569" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=LrTIp5IAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/19199811" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://publons.com/a/K-7989-2018/" title="Publons" rel="external nofollow noopener" target="_blank"><i class="ai ai-publons"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=57191960214" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://github.com/DmitryRyumin" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="/ru/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note">Лучше всего со мной можно связаться по электронной почте или в Telegram.</div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Авторское право 2025 к.т.н. Рюмин Дмитрий Александрович. Сайт создан с помощью <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> и темы <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. Размещен на <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Последнее обновление: 23 Марта, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?49d89e0621a24ef7e11392b1481e6bff"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Введите текст, чтобы начать поиск"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-\u043e\u0431\u043e-\u043c\u043d\u0435",title:"\u043e\u0431\u043e \u043c\u043d\u0435",section:"\u041c\u0435\u043d\u044e \u043d\u0430\u0432\u0438\u0433\u0430\u0446\u0438\u0438",handler:()=>{window.location.href="/ru/"}},{id:"nav-\u0431\u043b\u043e\u0433",title:"\u0431\u043b\u043e\u0433",description:"\u041f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 \u0442\u0440\u0435\u043d\u0434\u044b, \u0438\u0434\u0435\u0438 \u0438 \u0438\u043d\u043d\u043e\u0432\u0430\u0446\u0438\u0438 \u0432 \u0418\u0418 \u0438 \u041c\u041e",section:"\u041c\u0435\u043d\u044e \u043d\u0430\u0432\u0438\u0433\u0430\u0446\u0438\u0438",handler:()=>{window.location.href="/ru/blog/"}},{id:"nav-\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0446\u0438\u0438",title:"\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0446\u0438\u0438",description:"",section:"\u041c\u0435\u043d\u044e \u043d\u0430\u0432\u0438\u0433\u0430\u0446\u0438\u0438",handler:()=>{window.location.href="/ru/publications/"}},{id:"nav-\u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0438",title:"\u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0438",description:"",section:"\u041c\u0435\u043d\u044e \u043d\u0430\u0432\u0438\u0433\u0430\u0446\u0438\u0438",handler:()=>{window.location.href="/ru/repositories/"}},{id:"nav-\u0440\u0435\u0437\u044e\u043c\u0435",title:"\u0440\u0435\u0437\u044e\u043c\u0435",description:"",section:"\u041c\u0435\u043d\u044e \u043d\u0430\u0432\u0438\u0433\u0430\u0446\u0438\u0438",handler:()=>{window.location.href="/ru/cv/"}},{id:"post-\u0432\u0432\u0435\u0434\u0435\u043d\u0438\u0435-\u0432-\u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443-datasets",title:"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0432 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 Datasets",description:"\u041a\u0440\u0430\u0442\u043a\u043e\u0435 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e \u043e\u0441\u043d\u043e\u0432\u0430\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 Datasets",section:"\u0417\u0430\u043c\u0435\u0442\u043a\u0438",handler:()=>{window.location.href="/ru/blog/2024/intro-to-datasets/"}},{id:"post-\u0432\u0432\u0435\u0434\u0435\u043d\u0438\u0435-\u0432-\u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0435-\u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435-\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435",title:"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0432 \u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0435 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435",description:"\u041a\u0440\u0430\u0442\u043a\u043e\u0435 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e \u043e\u0441\u043d\u043e\u0432\u0430\u043c \u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0433\u043e \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f",section:"\u0417\u0430\u043c\u0435\u0442\u043a\u0438",handler:()=>{window.location.href="/ru/blog/2024/intro-to-deep-learning/"}},{id:"socials-email",title:"\u041e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435",section:"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0435\u0442\u0438",handler:()=>{window.open("mailto:%6E%65%77%65%72%61%61%69%72%65%73%65%61%72%63%68@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-telegram",title:"Telegram",section:"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0435\u0442\u0438",handler:()=>{window.open("https://telegram.me/dmitry_ryumin","_blank")}},{id:"socials-orcid",title:"ORCID",section:"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0435\u0442\u0438",handler:()=>{window.open("https://orcid.org/0000-0002-7935-0569","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0435\u0442\u0438",handler:()=>{window.open("https://scholar.google.com/citations?user=LrTIp5IAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0435\u0442\u0438",handler:()=>{window.open("https://www.semanticscholar.org/author/19199811","_blank")}},{id:"socials-publons",title:"Publons",section:"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0435\u0442\u0438",handler:()=>{window.open("https://publons.com/a/K-7989-2018/","_blank")}},{id:"socials-scopus",title:"Scopus",section:"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0435\u0442\u0438",handler:()=>{window.open("https://www.scopus.com/authid/detail.uri?authorId=57191960214","_blank")}},{id:"socials-github",title:"GitHub",section:"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0435\u0442\u0438",handler:()=>{window.open("https://github.com/DmitryRyumin","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0435\u0442\u0438",handler:()=>{window.open("/feed.xml","_blank")}},{id:"lang-en",title:"\u0410\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439",section:"\u042f\u0437\u044b\u043a\u0438",handler:()=>{window.location.href="/"}},{id:"light-theme",title:"\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0442\u0435\u043c\u0443 \u043d\u0430 \u0441\u0432\u0435\u0442\u043b\u0443\u044e",description:"\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0442\u0435\u043c\u0443 \u043d\u0430 \u0441\u0430\u0439\u0442\u0435 \u043d\u0430 \u0441\u0432\u0435\u0442\u043b\u0443\u044e",section:"\u0422\u0435\u043c\u044b",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0442\u0435\u043c\u0443 \u043d\u0430 \u0442\u0435\u043c\u043d\u0443\u044e",description:"\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0442\u0435\u043c\u0443 \u043d\u0430 \u0441\u0430\u0439\u0442\u0435 \u043d\u0430 \u0442\u0435\u043c\u043d\u0443\u044e",section:"\u0422\u0435\u043c\u044b",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u0438\u0441\u0442\u0435\u043c\u043d\u0443\u044e \u0442\u0435\u043c\u0443 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e",description:'\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0442\u0435\u043c\u0443 \u0441\u0430\u0439\u0442\u0430 \u043d\u0430 "\u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e"',section:"\u0422\u0435\u043c\u044b",handler:()=>{setThemeSetting("system")}}];</script> <script>function detectOS(){const n=window.navigator.userAgent,e=window.navigator.platform,t=["Win32","Win64","Windows","WinCE"],i=["iPhone","iPad","iPod"];let o="Unknown";return["Macintosh","MacIntel","MacPPC","Mac68K"].includes(e)?o="MacOS":i.includes(e)?o="iOS":t.includes(e)?o="Windows":/Android/.test(n)?o="Android":/Linux/.test(e)&&(o="Linux"),o}function detectLanguage(){return window.location.pathname.startsWith("/ru/")?"ru":"en"}function setHotkey(){const n={COMMAND_K:"&#x2318;+k",CTRL_K:"ctrl+k",TOUCH_TO_SEARCH_EN:"search",TOUCH_TO_SEARCH_RU:"\u043f\u043e\u0438\u0441\u043a"},e={MacOS:{en:n.COMMAND_K,ru:n.COMMAND_K},Windows:{en:n.CTRL_K,ru:n.CTRL_K},Linux:{en:n.CTRL_K,ru:n.CTRL_K},iOS:{en:n.TOUCH_TO_SEARCH_EN,ru:n.TOUCH_TO_SEARCH_RU},Android:{en:n.TOUCH_TO_SEARCH_EN,ru:n.TOUCH_TO_SEARCH_RU},Unknown:{en:n.CTRL_K,ru:n.CTRL_K}},t=detectOS(),i=detectLanguage();document.getElementById("search-hotkey").innerHTML=e[t][i]}setHotkey();</script> <script>document.addEventListener("DOMContentLoaded",function(){var t=window.location.pathname,n=t.split("/")[1],o=t.split("/")[2],e=document.querySelector(".post-title");e&&"Page not found"===e.textContent.trim()&&("ru"===n?window.location.href="/ru/404":"404"!==o&&(window.location.href="/en/404"))});</script> </body> </html>